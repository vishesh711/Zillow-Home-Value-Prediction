{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQV5S3FJPUjoK3sVmHUkHk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishesh711/Zillow-Home-Value-Prediction/blob/main/Multivariate_Time_Series_Forecasting_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c zillow-prize-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "388ZTFjpDTpI",
        "outputId": "4dc31c81-845a-4f3e-dd68-aded2b8c4e39"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 398, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multivariate Time Series Forecasting with LSTM\n",
        "Neural networks like Long Short-Term Memory (LSTM) recurrent neural networks are able to almost seamlessly model problems with multiple input variables. This is a great benefit in time series forecasting, where classical linear methods can be difficult to adapt to multivariate or multiple input forecasting problems."
      ],
      "metadata": {
        "id": "kME3nQb1_oOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import numpy as numpy\n",
        "import pandas as pd\n",
        "import pylab\n",
        "import calendar\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from sklearn import model_selection, preprocessing\n",
        "from scipy.stats import kendalltau\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "## Keras comes here\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout, BatchNormalization\n",
        "#from keras.layers.advanced_activations import PReLU\n",
        "from keras.optimizers import Adam\n",
        "#rom keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras import callbacks\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "pqgpoZAw_nuA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lOzawaH_l0-",
        "outputId": "d8c36353-5e65-46b1-d1ff-4a5da8eeba87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train, prop and sample data\n"
          ]
        }
      ],
      "source": [
        "print('Loading train, prop and sample data')\n",
        "train = pd.read_csv(\"train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
        "prop = pd.read_csv('properties_2016.csv')\n",
        "sample = pd.read_csv('sample_submission.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoder\n",
        "LabelEncoder is a utility class to help normalize labels categorical values and to encode such that they contain only values between 0 and n_classes-1.\n",
        "\n",
        "Here, we LabelEncode the properties dataset."
      ],
      "metadata": {
        "id": "9DVrKOTYA3Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Fitting Label Encoder on properties')\n",
        "for c in prop.columns:\n",
        "    prop[c]=prop[c].fillna(-1)\n",
        "    if prop[c].dtype == 'object':\n",
        "        lbl = LabelEncoder()\n",
        "        lbl.fit(list(prop[c].values))\n",
        "        prop[c] = lbl.transform(list(prop[c].values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtefqKM7AM76",
        "outputId": "3029d6b4-294a-4d2a-da0a-c3d37c7387d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting Label Encoder on properties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Creating training set:')\n",
        "df_train = train.merge(prop, how='left', on='parcelid')\n",
        "\n",
        "print('Creating df_test  :')\n",
        "sample['parcelid'] = sample['ParcelId']\n",
        "\n",
        "print(\"Merge Sample with property data :\")\n",
        "df_test = sample.merge(prop, on='parcelid', how='left')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5aCDgsmA6NA",
        "outputId": "9277d389-4098-40aa-bba6-64c31ccccaa6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training set:\n",
            "Creating df_test  :\n",
            "Merge Sample with property data :\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n",
        "df_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\n",
        "\n",
        "basedate = pd.to_datetime('2015-11-15').toordinal()\n",
        "df_train['cos_season'] = \\\n",
        "        ( (pd.to_datetime(df_train['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
        "          (2*np.pi/365.25) ).apply(np.cos)\n",
        "df_train['sin_season'] = \\\n",
        "        ( (pd.to_datetime(df_train['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n",
        "          (2*np.pi/365.25) ).apply(np.sin)\n",
        "\n",
        "#test dataset\n",
        "df_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')\n",
        "df_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\n",
        "\n",
        "df_test['cos_season'] = np.cos( (pd.to_datetime('2016-11-15').toordinal() - basedate) * \\\n",
        "                                    (2*np.pi/365.25) )\n",
        "df_test['sin_season'] = np.sin( (pd.to_datetime('2016-11-15').toordinal() - basedate) * \\\n",
        "                                    (2*np.pi/365.25) )\n",
        "\n",
        "df_train_x = df_train.drop(['logerror','parcelid', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode',\n",
        "                             'fireplacecnt', 'fireplaceflag'],axis=1)\n",
        "df_train = df_train.drop(['parcelid', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode',\n",
        "                             'fireplacecnt', 'fireplaceflag'], axis=1)\n",
        "\n",
        "train_columns = df_train_x.columns\n",
        "df_test=df_test[train_columns]"
      ],
      "metadata": {
        "id": "prih3upTA8n-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM Data Preparation\n",
        "The first step is to prepare properties dataset for LSTM.This involves framing the dataset as a supervised learning problem and normalizing the input variables. We will frame the supervised learning problem as predicting the Log Error for a particular parcel Id given other features and conditions at the prior time step.\n",
        "\n",
        "We can transform the dataset using the series_to_supervised() function that is developed below;"
      ],
      "metadata": {
        "id": "fco7z8ajBCJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "eK8jcu5tBAL_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Values = df_train.values\n",
        "test_values = df_test.values\n",
        "values = Values.astype('float32')\n",
        "test_values = test_values.astype('float32')\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(values)\n",
        "scaled_test = scaler.fit_transform(test_values)\n",
        "reframed = series_to_supervised(scaled, 1, 1)\n",
        "reframed_test = series_to_supervised(scaled,1,1)\n",
        "reframed.drop(reframed.columns[58:116], axis=1, inplace=True)\n",
        "reframed_test.drop(reframed_test.columns[56:111],axis =1,inplace=True)\n",
        "\n",
        "train_X, train_Y = reframed.iloc[:80000,:-1], reframed.iloc[:80000,-1]\n",
        "valid_X, valid_Y = reframed.iloc[80000:,:-1], reframed.iloc[80000:,-1]\n",
        "\n",
        "train_X = np.array(train_X)\n",
        "train_Y = np.array(train_Y)\n",
        "\n",
        "valid_X = np.array(valid_X)\n",
        "valid_Y = np.array(valid_Y)\n",
        "test_X = np.array(reframed_test)"
      ],
      "metadata": {
        "id": "N8CcHgqDBHLZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "valid_X = valid_X.reshape((valid_X.shape[0], 1, valid_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "print(train_X.shape, train_Y.shape, valid_X.shape, valid_Y.shape,test_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ORfQjCnBJlN",
        "outputId": "cd400ead-d4e7-4cbf-fae0-0ee7b2847e9a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(956, 1, 57) (956,) (0, 1, 57) (0,) (956, 1, 59)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential()\n",
        "# model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "# model.add(Dense(1))\n",
        "# model.compile(loss='mae', optimizer='adam')\n",
        "# # fit network\n",
        "# model.fit(train_X, train_Y, epochs=50, batch_size=72, validation_data=(valid_X, valid_Y), verbose=2, shuffle=False)\n",
        "# yhat = model.predict(valid_X)\n",
        "# valid_X = valid_X.reshape((valid_X.shape[0], valid_X.shape[2]))\n",
        "\n",
        "# rmse = np.sqrt(mean_squared_error(valid_Y, yhat))\n",
        "# print('Test RMSE: %.3f' % rmse)\n",
        "\n",
        "# Print the shapes of the datasets to ensure they are correct and not empty\n",
        "print('Train X shape:', train_X.shape)\n",
        "print('Train Y shape:', train_Y.shape)\n",
        "print('Valid X shape:', valid_X.shape)\n",
        "print('Valid Y shape:', valid_Y.shape)\n",
        "\n",
        "# Ensure the content is not empty\n",
        "print('First row of Train X:', train_X[0] if train_X.shape[0] > 0 else 'Empty')\n",
        "print('First row of Train Y:', train_Y[0] if train_Y.shape[0] > 0 else 'Empty')\n",
        "print('First row of Valid X:', valid_X[0] if valid_X.shape[0] > 0 else 'Empty')\n",
        "print('First row of Valid Y:', valid_Y[0] if valid_Y.shape[0] > 0 else 'Empty')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw1mebzUBNkS",
        "outputId": "70b77b25-3686-46ab-e426-b02ffce4e034"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X shape: (956, 1, 57)\n",
            "Train Y shape: (956,)\n",
            "Valid X shape: (0, 1, 57)\n",
            "Valid Y shape: (0,)\n",
            "First row of Train X: [[0.49185398 0.         0.         0.         0.25       0.30769232\n",
            "  0.         0.         0.3181818  0.         0.         0.14987291\n",
            "  0.14987291 0.         0.         0.         0.         0.29729462\n",
            "  0.27272728 0.33333334 0.19703574 0.         0.         0.38352585\n",
            "  0.8412552  0.00200633 0.         0.         0.         0.\n",
            "  0.         0.9426229  0.29645538 0.06690589 0.         0.\n",
            "  0.24286923 0.5833334  0.         0.6666667  0.         0.\n",
            "  0.         0.         0.9747024  0.75       0.01924738 0.02633956\n",
            "  0.         0.02744794 0.0170863  0.         0.         0.9914912\n",
            "  0.         0.8261748  0.8789671 ]]\n",
            "First row of Train Y: 0.4611325\n",
            "First row of Valid X: Empty\n",
            "First row of Valid Y: Empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "# Fit the network\n",
        "model.fit(train_X, train_Y, epochs=50, batch_size=72, validation_data=(valid_X, valid_Y), verbose=2, shuffle=False)\n",
        "\n",
        "# Make predictions\n",
        "yhat = model.predict(valid_X)\n",
        "valid_X = valid_X.reshape((valid_X.shape[0], valid_X.shape[2]))\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(valid_Y, yhat))\n",
        "print('Test RMSE: %.3f' % rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "iXhaaYggERer",
        "outputId": "75172a32-ce80-4f75-c8c4-96d35ca9c473"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input data to be non-empty.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-643da9ec5c4c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Fit the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m72\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected input data to be non-empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     def _configure_dataset_and_inferred_steps(\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input data to be non-empty."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Summary:\n",
        "In this notebook, we have implemented Simple neural networks with 5 layers for prediction of LogError = (log(Zestimate)-log(salesprice)) using 2016 property dataset and its corresponding log error values provided by zillow for home value prediction in Python using Keras and tensorflow deep learning libraries.\n",
        "\n",
        "Finally, we have predicted logerror values of 2016 and 2017 for the last quarter (from November to December) in the test dataset. Calculated RMSE for the Network built can be seen as 0.017 which is improved from that which we got by simple neural network. we can aslo infer that there is minimal error in the logerror gives us the better predictions.Further, the model can improvised by add more layers or changing the backpropagation parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "3hnLXNTpBRLN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FeyFbKthBSEM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}